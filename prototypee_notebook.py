# -*- coding: utf-8 -*-
"""Prototypee_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mJNFooEAivStOkCWwLWZPf0MDuGeqmMd

# Anime Recommender Project

![image.png](attachment:image.png)

Table of contents:
* [1. Project Overview](#chapter1)
    - [1.1. Introduction](#section_1_1)
    - [1.2. Problem Statement](#section_1_2)
    - [1.3. Objective of the Project](#section_1_3)
    - [1.4. Data Source](#section_1_4)
    - [1.5. Importance of the Study](#section_1_5)
    - [1.6. Key Questions or Hypotheses](#section_1_6)
    - [1.7. Methodology Overview](#section_1_7)
* [2. Importing Packages](#chapter2)
* [3. Loading Data](#chapter3)
* [4. Data Cleaning](#chapter4)
* [5. Exploratory Data Analysis (EDA))](#chapter5)
    - [5.1. Category Distribution](#section_5_1)
    - [5.2. Text Length Analysis](#section_5_2)
    - [5.3. Text Length Distribution by Category](#section_5_3)
    - [5.4. WordCloud](#section_5_4)
* [6. Data Preprocessing](#chapter6)
    - [6.1 Preprocessing Tasks for Classification](#section_6_1)
* [7. Modelling](#chapter7)
* [8. Model evaluation metrics](#chapter8)
* [9. Model performance analysis](#chapter9)
* [10. Conclusion](#chapter10)
* [11. References](#chapter11)

## 1. Project Overview <a class="anchor" id="chapter1"></a>

#### 1.1. Introduction <a class="anchor" id="section_1_1"></a>
-

#### 1.2. Problem Statement: <a id="section_1_2"></a>
-

#### 1.3. Objective of the Project <a id="section_1_3"></a>
-

#### 1.4. Data Source <a id="section_1_4"></a>
- The dataset used in this project comprises of three cvs namely anime.csv, test.csv and train.csv which contain a diverse collection of anime movies, each tagged with a specific category.
- The dataset includes multiple fields for each movie:

#### 1.5. Importance of the Study <a id="section_1_5"></a>.
-


#### 1.6. Key Questions or Hypotheses <a id="section_1_6"></a>

#### 1.7. Methodology Overview <a id="section_1_7"></a>
The methodology for this project includes several key steps:
- **Data Cleaning:** This step involves filtering out non-essential content to remove irrelevant information, managing incomplete data entries by handling missing values, standardizing the text format by normalizing text (e.g., converting to lowercase), and eliminating non-alphanumeric characters along with common, insignificant words through the removal of special characters and stop words.
- **Data Preprocessing:** This step involves feature extraction which is performed to transform raw text data into meaningful numerical representations. Techniques used include Term Frequency-Inverse Document Frequency (TF-IDF) for evaluating word importance in documents, word embeddings (such as Word2Vec and GloVe) for mapping words in vector space, and tokenization, which splits text into individual tokens for further analysis.
- **Exploratory Data Analysis (EDA):** This step involves analyzing the dataset to uncover patterns, trends, and relationships within the data. This step includes visualizing the distribution of categories, word frequency analysis, and other statistical measures.
- **Model Building:** This step involves training various machine learning models, including traditional algorithms like Logistic Regression, Support Vector Machines (SVM), and Random Forest, as well as advanced models like Convolutional Neural Networks (CNN) and Long Short-Term Memory networks (LSTM).
- **Model Evaluation:** This step involves assessing the performance of the models using metrics such as accuracy, precision, recall, and F1-score. This step involves fine-tuning hyperparameters and selecting the best-performing model based on the evaluation metrics.
- **Tools and Libraries:** Utilizing Python and its libraries, including Pandas for data manipulation, Numpy for numerical operations, Scikit-learn for machine learning algorithms, and TensorFlow/Keras for deep learning models.

## 2. Importing Packages <a class="anchor" id="chapter2"></a>
+ For data loading, manipulation, and analysis: `Pandas`, `csv`, `string`, `re`, `nltk`, `wordcloud` and `Numpy`.
+ For data visualization: `Matplotlib` and `Seaborn`

<div class="alert alert-block alert-info">
These libraries and tools collectively provide a comprehensive set of capabilities for handling data (pandas, numpy), manipulating text (re, nltk), and performing advanced natural language processing tasks (nltk). They are widely used in data science, machine learning, and text analytics projects due to their efficiency and versatility.
"""

# Commented out IPython magic to ensure Python compatibility.
# Libraries for data loading, manipulation and analysis
from wordcloud import WordCloud
import numpy as np
import pandas as pd
import csv
import seaborn as sns
import matplotlib.pyplot as plt
import string
import re
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from collections import Counter
from nltk.probability import FreqDist
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from scipy.stats import uniform, randint
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import PCA
import mlflow
import mlflow.sklearn
from sklearn.manifold import TSNE, MDS
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.base import BaseEstimator, TransformerMixin
import pickle
import os
# Displays output inline
# %matplotlib inline

# Libraries for Handing Errors
import warnings
warnings.filterwarnings('ignore')

# Create the .kaggle directory
os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)

# Move kaggle.json to the .kaggle directory
!mv kaggle.json ~/.kaggle/

# Set permissions for the kaggle.json file
!chmod 600 ~/.kaggle/kaggle.json

"""## 3. Loading Data <a class="anchor" id="chapter3"></a>

The data used for this project was located in the Data folder which contains three files `anime.csv`  `test.csv` and `train.csv`. To better manipulate and analyse the `anime.csv`, `test.csv` and `train.csv` files, it was loaded into a Pandas Data Frame using the Pandas function, `pd.read_csv()` and referred to as `train_data` and `test_data`. For demonstrating the column index in the dataframe , `index_col=False` was implemented.
"""

# Download the competition dataset
!kaggle competitions download -c anime-recommender-system-project-2024
!unzip anime-recommender-system-project-2024.zip

# Loading the data
anime_df = pd.read_csv('anime.csv')
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

print(f' Anime Dataset shape: {anime_df.shape}')
print(f' Train Dataset shape: {train_df.shape}')
print(f' Test Dataset shape: {test_df.shape}')

print(f' Anime Dataset Information:\n{anime_df.info()}\n')
print(f' Train Dataset Information:\n{train_df.info()}\n')
print(f' Test Dataset Information:\n{test_df.info()}\n')

print(f' Anime Dataset Information:\n{anime_df.describe()}\n')
print(f' Train Dataset Information:\n{train_df.describe()}\n')
print(f' Test Dataset Information:\n{test_df.describe()}\n')

"""### 3.1 Exploring the data"""

def analyze_dataframe(df, name):
    """
    Analyzes a DataFrame for:
    1. Displaying the first few rows
    2. Checking for 'Unknown' values
    3. Checking for duplicate rows
    4. Checking for missing values
    """
    # Display the first few rows
    print(f"First few rows of {name} DataFrame:")
    print(df.head())
    print("\n")

    # Check for 'Unknown' values (case insensitive) in each column
    unknown_values = (df.applymap(lambda x: str(x).lower() == 'unknown' if pd.notnull(x) else False)).sum()
    print(f"'Unknown' values in each column of {name} DataFrame:")
    print(unknown_values)
    print("\n")

    # Check for duplicate rows
    duplicate_rows = df.duplicated().sum()
    print(f"Number of duplicate rows in {name} DataFrame: {duplicate_rows}")
    print("\n")

    # Check for missing values
    missing_values = df.isnull().sum()
    print(f"Missing values in {name} DataFrame:")
    print(missing_values)
    print("\n")

# Analyze each DataFrame
analyze_dataframe(anime_df, "anime")
analyze_dataframe(train_df, "train")
analyze_dataframe(test_df, "test")

def check_negative_values(df, skip_columns):
    """
    Checks for negative values in the DataFrame while skipping specified columns.

    Parameters:
    - df: The DataFrame to check.
    - skip_columns: List of column names to skip in the negative value check.

    Returns:
    - A Series with the count of negative values for each column (excluding skipped columns).
    """
    # Create a mask for columns that should be checked (excluding skip_columns)
    check_columns = [col for col in df.columns if col not in skip_columns]

    # Apply the negative value check to columns that are not skipped
    negative_values = (df[check_columns].applymap(lambda y: float(y) < 0 if pd.notnull(y) else False)).sum()

    return negative_values

# Columns to skip
skip_columns = ['genre', 'type', 'name', 'episodes']

print(f'Anime_df negative values:\n{check_negative_values(anime_df, skip_columns)}\n')
print(f'Train_df negative values:\n{check_negative_values(train_df, skip_columns)}\n')
print(f'Test_df negative values:\n{check_negative_values(test_df, skip_columns)}\n')

"""## 4. Data Cleaning <a class="anchor" id="chapter4"></a>
Data cleaning is a crucial step in the data analysis process, involving the correction or removal of incorrect, corrupted, duplicate, or incomplete data within a dataset. Through various techniques such as filling missing values, removing outliers, and standardizing data formats, it ensures the accuracy and reliability of subsequent analyses and decision-making.
"""

# Handle missing values in the anime dataset
import pandas as pd

# Assuming anime_df, train_df, and test_df are already loaded

def clean_data(df):
    # Drop rows with any missing values
    df_cleaned = df.dropna()

    # Remove rows containing 'unknown' in any column
    for column in df_cleaned.columns:
        df_cleaned = df_cleaned[df_cleaned[column] != 'Unknown']

        # Drop duplicate rows
    df_cleaned = df_cleaned.drop_duplicates()

    return df_cleaned

# Apply the cleaning function to each dataset
anime_df_cleaned = clean_data(anime_df)
train_df_cleaned = clean_data(train_df)
test_df_cleaned = clean_data(test_df)

# Display the cleaned data
print("Cleaned anime_df:")
print(anime_df_cleaned.head())
print("\nCleaned train_df:")
print(train_df_cleaned.head())
print("\nCleaned test_df:")
print(test_df_cleaned.head())

# Verify data cleaning
analyze_dataframe(anime_df_cleaned, "anime")
analyze_dataframe(train_df_cleaned, "train")
analyze_dataframe(test_df_cleaned, "test")

"""## 5. Exploratory Data Analysis (EDA) <a class="anchor" id="chapter5"></a>


To give a better understanding of the variables and the relationships between them, we set out to do an **Exploratory Data Analysis (EDA)** of our dataset. The main tasks includes investigating and summarizing the dataframe's main characteristics by data visualization methods and statistical analyses. Furthermore, investigating the dataset’s key features, summarizing its central characteristics, and employing both data visualisation techniques and statistical analyses to draw meaningful insights that can guide further research and data-driven decision making.
"""

# Load Cleaned Datasets
anime_df = pd.read_csv('cleaned_anime.csv')
train_df = pd.read_csv('cleaned_train.csv')
test_df = pd.read_csv('cleaned_test.csv')

print(train_df['rating'].describe())
print(anime_df['rating'].describe())

top_anime_by_members = anime_df.sort_values(by='members', ascending=False).head(20)
plt.figure(figsize=(12, 6))
sns.barplot(x='members', y='name', data=top_anime_by_members)
plt.title('Top 20 Most Popular Anime by Members')
plt.xlabel('Number of Members')
plt.ylabel('Anime Name')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(anime_df['rating'], bins=20, kde=True)
plt.title('Distribution of Anime Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(y='type', data=anime_df, order=anime_df['type'].value_counts().index)
plt.title('Number of Anime by Type')
plt.xlabel('Count')
plt.ylabel('Type')
plt.show()

genres = anime_df['genre'].str.split(', ', expand=True).stack().value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=genres.values, y=genres.index)
plt.title('Top 10 Genres by Number of Anime')
plt.xlabel('Number of Anime')
plt.ylabel('Genre')
plt.show()

"""### Train CVS EDA"""

plt.figure(figsize=(12, 6))
sns.histplot(train_df['rating'], bins=10, kde=True)
plt.title('Distribution of User Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

user_avg_ratings = train_df.groupby('user_id')['rating'].mean()
plt.figure(figsize=(12, 6))
sns.histplot(user_avg_ratings, bins=20, kde=True)
plt.title('Distribution of Average Rating per User')
plt.xlabel('Average Rating')
plt.ylabel('Count')
plt.show()

"""### Test Dataset EDA"""

test_user_counts = test_df['user_id'].value_counts()
plt.figure(figsize=(12, 6))
sns.histplot(test_user_counts, bins=20, kde=True)
plt.title('Number of Anime Rated by Users in Test Set')
plt.xlabel('Number of Anime Rated')
plt.ylabel('Count')
plt.show()

"""## 6. Data Preprocessing <a class="anchor" id="chapter6"></a>

This section covers the data preprocessing, which involves preparing text data for analysis by removing errors and inconsistencies. It includes downloading NLTK packages, loading datasets, and cleaning the text by removing noise, punctuation,converting to lowercase, and removing contractions. The text is then tokenized, stop words are removed, and words are stemmed and lemmatized. Finally, the processed text is reassembled into strings, with an option to save the cleaned datasets to CSV files.
"""

# Download required NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def clean_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Clean the 'name' and 'genre' columns for train and test datasets
anime_df_cleaned[['name', 'genre']] = anime_df_cleaned[['name', 'genre']].applymap(clean_text)

anime_df_cleaned

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, normalize
from scipy.sparse import csr_matrix, hstack

# Assuming anime_df_cleaned is already loaded
# Dropping unnecessary columns for content-based filtering
content_df = anime_df_cleaned.drop(columns=['anime_id'])

# TF-IDF vectorizer for the genre
tfidf = TfidfVectorizer()
genre_matrix = tfidf.fit_transform(content_df['genre'])

# Combine genre and other numerical features
additional_features = content_df[['rating', 'episodes', 'members']].values

# Standard scale the additional features
scaler = StandardScaler()
additional_features_scaled = scaler.fit_transform(additional_features)

# Convert scaled additional features to sparse matrix
additional_features_sparse = csr_matrix(additional_features_scaled)

# Combine genre matrix and scaled additional features
combined_features = hstack([genre_matrix, additional_features_sparse])

# Normalize combined features
combined_features = normalize(combined_features, axis=1)

"""#### Dimensionality Reduction Using PCA"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix, hstack

# Assuming anime_df_cleaned is already loaded
# Dropping unnecessary columns for content-based filtering
content_df = anime_df_cleaned.drop(columns=['anime_id'])

# TF-IDF vectorizer for the genre
tfidf = TfidfVectorizer()
genre_matrix = tfidf.fit_transform(content_df['genre'])

# Combine genre and other numerical features
additional_features = content_df[['rating', 'episodes', 'members']].values

# Standard scale the additional features
scaler = StandardScaler()
additional_features_scaled = scaler.fit_transform(additional_features)

# Convert scaled additional features to sparse matrix
additional_features_sparse = csr_matrix(additional_features_scaled)

# Grid search for PCA
pca = PCA(svd_solver='randomized')
pipeline_pca = Pipeline([('pca', pca)])
param_grid_pca = {'pca__n_components': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}
search_pca = GridSearchCV(pipeline_pca, param_grid_pca, cv=5)
search_pca.fit(combined_features.toarray())
best_n_components = search_pca.best_params_['pca__n_components']
print(f"Best number of PCA components: {best_n_components}")

# Apply PCA with the best number of components
pca = PCA(n_components=best_n_components, svd_solver='randomized')
combined_features_reduced_pca = pca.fit_transform(combined_features.toarray())

# Calculate cosine similarity between items using PCA-reduced features
cosine_sim_pca = cosine_similarity(combined_features_reduced_pca)
print("Cosine similarity matrix calculated successfully for PCA.")

"""#### Dimensionality Reduction Using t-SNE"""

import numpy as np
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from scipy.sparse import csr_matrix, hstack

# Define parameter grid for t-SNE
param_grids_tsne = [
    {'perplexity': 5, 'n_iter': 300},
    {'perplexity': 5, 'n_iter': 1000},
    {'perplexity': 30, 'n_iter': 300},
    {'perplexity': 30, 'n_iter': 1000},
    {'perplexity': 50, 'n_iter': 300},
    {'perplexity': 50, 'n_iter': 1000}
]

# Function to perform manual grid search and return best parameters and score
def perform_manual_grid_search(method_class, param_grids, combined_features):
    best_score = np.inf
    best_params = None
    best_embedding = None

    for params in param_grids:
        method = method_class(**params)
        transformed = method.fit_transform(combined_features.toarray())

        # Evaluate the quality of the embedding using k-means inertia
        kmeans = KMeans(n_clusters=10, random_state=0)
        kmeans.fit(transformed)
        score = kmeans.inertia_

        if score < best_score:
            best_score = score
            best_params = params
            best_embedding = transformed

    return best_params, best_embedding

# Grid search for t-SNE
best_params_tsne, combined_features_tsne_reduced = perform_manual_grid_search(
    TSNE, param_grids_tsne, combined_features
)
print(f"Best parameters for t-SNE: {best_params_tsne}")

# Calculate cosine similarity between items using t-SNE-reduced features
cosine_sim_tsne = cosine_similarity(combined_features_tsne_reduced)
print("Cosine similarity matrix calculated successfully for t-SNE.")

"""## 7. Modelling <a class="anchor" id="chapter7"></a>

Modelling is the process of creating and using a simplified representation or abstraction of a real-world system or phenomenon. It involves using mathematical, statistical, or computational techniques to simulate, predict, or understand the behavior of complex systems. Modelling aims to capture the essential features and relationships of the system under study, allowing for analysis, prediction, and decision-making based on the model's outputs.

### 7.1 Content based Model Training
"""

# Assuming anime_df_cleaned is already loaded
# Dropping unnecessary columns for content-based filtering
content_df = anime_df_cleaned.drop(columns=['anime_id'])

# TF-IDF vectorizer for the genre
tfidf = TfidfVectorizer()
genre_matrix = tfidf.fit_transform(content_df['genre'])

# Combine genre and other numerical features
additional_features = content_df[['rating', 'episodes', 'members']].values

# Standard scale the additional features
scaler = StandardScaler()
additional_features_scaled = scaler.fit_transform(additional_features)

# Convert scaled additional features to sparse matrix
additional_features_sparse = csr_matrix(additional_features_scaled)

# Combine genre matrix and scaled additional features
combined_features = hstack([genre_matrix, additional_features_sparse])

# Normalize combined features
combined_features = normalize(combined_features, axis=1)

"""#### Model training with PCA"""

# Apply PCA with the best number of components
pca = PCA(n_components=20, svd_solver='randomized')
combined_features_reduced_pca = pca.fit_transform(combined_features.toarray())

# Calculate cosine similarity between items using PCA-reduced features
cosine_sim_pca = cosine_similarity(combined_features_reduced_pca)
print("Cosine similarity matrix calculated successfully for PCA.")

from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.cluster import KMeans

# Train KMeans on PCA-reduced features
kmeans_pca = KMeans(n_clusters=10, random_state=42)
kmeans_pca.fit(combined_features_reduced_pca)

# Predict the clusters
labels_pca = kmeans_pca.labels_

# Calculate the silhouette score
silhouette_avg_pca = silhouette_score(combined_features_reduced_pca, labels_pca)

# Calculate the Davies-Bouldin index
db_index_pca = davies_bouldin_score(combined_features_reduced_pca, labels_pca)

print("For PCA with n_components=20:")
print(f"The average silhouette score is: {silhouette_avg_pca}")
print(f"The Davies-Bouldin index is: {db_index_pca}")

"""#### Model training with t-SNE"""

from sklearn.manifold import TSNE

# Apply t-SNE with the best parameters
tsne = TSNE(perplexity=50, n_iter=300)
combined_features_reduced_tsne = tsne.fit_transform(combined_features.toarray())

# Calculate cosine similarity between items using t-SNE-reduced features
cosine_sim_tsne = cosine_similarity(combined_features_reduced_tsne)
print("Cosine similarity matrix calculated successfully for t-SNE.")

from sklearn.metrics import silhouette_score, mean_squared_error, davies_bouldin_score

# Train KMeans on t-SNE-reduced features
kmeans_tsne = KMeans(n_clusters=10, random_state=42)
kmeans_tsne.fit(combined_features_reduced_tsne)

# Predict the clusters
labels_tsne = kmeans_tsne.labels_

# Calculate the silhouette score
silhouette_avg_tsne = silhouette_score(combined_features_reduced_tsne, labels_tsne)

# Calculate the Davies-Bouldin index
db_index_tsne = davies_bouldin_score(combined_features_reduced_tsne, labels_tsne)

print("For t-SNE-reduced features:")
print(f"The average silhouette score is: {silhouette_avg_tsne}")
print(f"The Davies-Bouldin index is: {db_index_tsne}")

"""#### Hypertuning With RandomSearchCV and Mlflow"""

from sklearn.model_selection import RandomizedSearchCV

# Define parameter grid for RandomizedSearchCV
param_grid_kmeans = {
    'n_clusters': [5, 10, 15, 20, 25],
    'init': ['k-means++', 'random'],
    'max_iter': [300, 600, 900],
    'n_init': [10, 20, 30],
    'random_state': [42]
}

# Set up RandomizedSearchCV
random_search_kmeans = RandomizedSearchCV(KMeans(), param_distributions=param_grid_kmeans, n_iter=10, cv=5, random_state=42)

# Train KMeans with RandomizedSearchCV
random_search_kmeans.fit(combined_features_reduced_pca)

# Get the best KMeans model
best_kmeans = random_search_kmeans.best_estimator_

print(f'Best kmean hyperparameters: {best_kmeans}')

"""#### Applying The Paramaters"""

# Train KMeans on PCA-reduced features
kmeans_pca = KMeans(max_iter=900, n_clusters=25, n_init=30, random_state=42)
kmeans_pca.fit(combined_features_reduced_pca)

# Predict the clusters
labels_pca = kmeans_pca.labels_

# Calculate the silhouette score
silhouette_avg_pca = silhouette_score(combined_features_reduced_pca, labels_pca)

# Calculate the Davies-Bouldin index
db_index_pca = davies_bouldin_score(combined_features_reduced_pca, labels_pca)

print("For PCA with n_components=20:")
print(f"The average silhouette score is: {silhouette_avg_pca}")
print(f"The Davies-Bouldin index is: {db_index_pca}")

from sklearn.metrics import silhouette_score, mean_squared_error, davies_bouldin_score

# Train KMeans on t-SNE-reduced features
kmeans_tsne = KMeans(max_iter=900, n_clusters=25, n_init=30, random_state=42)
kmeans_tsne.fit(combined_features_reduced_tsne)

# Predict the clusters
labels_tsne = kmeans_tsne.labels_

# Calculate the silhouette score
silhouette_avg_tsne = silhouette_score(combined_features_reduced_tsne, labels_tsne)

# Calculate the Davies-Bouldin index
db_index_tsne = davies_bouldin_score(combined_features_reduced_tsne, labels_tsne)

print("For t-SNE-reduced features:")
print(f"The average silhouette score is: {silhouette_avg_tsne}")
print(f"The Davies-Bouldin index is: {db_index_tsne}")

import pickle
from sklearn.metrics.pairwise import cosine_similarity

# Content-Based Recommender Function
def recommend_anime_by_name(anime_name, kmeans_model, features_reduced, anime_df, num_recommendations=10):
    # Get the index of the anime
    anime_index = anime_df[anime_df['name'] == anime_name].index[0]

    # Predict the cluster for the given anime
    cluster_label = kmeans_model.predict(features_reduced[anime_index].reshape(1, -1))[0]

    # Get all anime indices belonging to the same cluster
    cluster_indices = np.where(kmeans_model.labels_ == cluster_label)[0]

    # Compute the cosine similarity matrix for the cluster
    cluster_features = features_reduced[cluster_indices]
    cosine_sim = cosine_similarity(cluster_features)

    # Get similarity scores for all animes in the same cluster
    sim_scores = list(enumerate(cosine_sim[0]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the top most similar animes
    sim_scores = sim_scores[1:num_recommendations + 1]
    anime_indices = [i[0] for i in sim_scores]

    # Return the recommended animes
    return anime_df.iloc[anime_indices]

# Example usage
anime_name = "naruto"  # Replace with the title of your choice
recommended_animes_tsne = recommend_anime_by_name(anime_name, kmeans_tsne, combined_features_reduced_tsne, anime_df_cleaned, num_recommendations=10)
print(recommended_animes_tsne[['name', 'genre', 'rating']])

import mlflow
import mlflow.sklearn
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
import numpy as np

# Apply PCA for dimensionality reduction
pca = PCA(n_components=20, svd_solver='randomized')
combined_features_reduced_pca = pca.fit_transform(combined_features.toarray())

# Define parameter grid for RandomizedSearchCV
param_grid_kmeans = {
    'n_clusters': [5, 10, 15, 20, 25],
    'init': ['k-means++', 'random'],
    'max_iter': [300, 600, 900],
    'n_init': [10, 20, 30],
    'random_state': [42]
}

# Set up RandomizedSearchCV
random_search_kmeans = RandomizedSearchCV(KMeans(), param_distributions=param_grid_kmeans, n_iter=10, cv=5, random_state=42)

# Start MLflow run
with mlflow.start_run():
    # Train KMeans with RandomizedSearchCV
    random_search_kmeans.fit(combined_features_reduced_pca)

    # Get the best KMeans model
    best_kmeans = random_search_kmeans.best_estimator_

    # Predict the clusters
    labels_pca = best_kmeans.labels_

    # Calculate the silhouette score
    silhouette_avg_pca = silhouette_score(combined_features_reduced_pca, labels_pca)

    # Calculate the Davies-Bouldin index
    db_index_pca = davies_bouldin_score(combined_features_reduced_pca, labels_pca)

    # Calculate the RMSE
    cosine_sim_reduced_pca = cosine_similarity(combined_features_reduced_pca)
    cosine_sim_original = cosine_similarity(combined_features.toarray())
    rmse_pca = np.sqrt(mean_squared_error(cosine_sim_original, cosine_sim_reduced_pca))

    # Log parameters and metrics using MLflow
    mlflow.log_params(random_search_kmeans.best_params_)
    mlflow.log_metric("silhouette_score", silhouette_avg_pca)
    mlflow.log_metric("davies_bouldin_index", db_index_pca)
    mlflow.log_metric("rmse", rmse_pca)

    # Log the model
    mlflow.sklearn.log_model(best_kmeans, "kmeans_model")

    print("For PCA with n_components=20 and KMeans clustering:")
    print(f"The average silhouette score is: {silhouette_avg_pca}")
    print(f"The Davies-Bouldin index is: {db_index_pca}")
    print(f"The RMSE is: {rmse_pca}")

"""#### 7.2. User-based Collaborative Filtering"""

print(train_df.head())
print(train_df.info())

analyze_dataframe(train_df_cleaned, "train")

import pandas as pd
from surprise import Dataset, Reader
from surprise.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import pairwise_distances, silhouette_score, davies_bouldin_score
from sklearn.cluster import AgglomerativeClustering
from scipy.sparse import csr_matrix, save_npz, load_npz
import numpy as np

# Define a reader for the dataset
reader = Reader(rating_scale=(train_df_cleaned.rating.min(), train_df_cleaned.rating.max()))

# Load the dataset from the DataFrame
dataset = Dataset.load_from_df(train_df_cleaned[['user_id', 'anime_id', 'rating']], reader)

# Split the dataset into training and testing sets
trainset, testset = train_test_split(dataset, test_size=0.2)

# Convert trainset to a DataFrame
trainset_df = pd.DataFrame(trainset.all_ratings(), columns=['user_id', 'anime_id', 'rating'])

# Create sparse matrix directly
user_ids = trainset_df['user_id'].astype('category').cat.codes
anime_ids = trainset_df['anime_id'].astype('category').cat.codes
ratings = trainset_df['rating'].values
item_user_matrix_sparse = csr_matrix((ratings, (anime_ids, user_ids)))

# Normalize the sparse matrix
scaler = StandardScaler(with_mean=False)  # with_mean=False to keep it suitable for sparse matrices
item_user_matrix_scaled = scaler.fit_transform(item_user_matrix_sparse)

# Compute the distance matrix in chunks to manage memory usage
chunk_size = 1000  # Adjust based on your system's memory capacity
distance_matrix = np.zeros((item_user_matrix_scaled.shape[0], item_user_matrix_scaled.shape[0]))

for start_row in range(0, item_user_matrix_scaled.shape[0], chunk_size):
    end_row = min(start_row + chunk_size, item_user_matrix_scaled.shape[0])
    distance_chunk = pairwise_distances(item_user_matrix_scaled[start_row:end_row], item_user_matrix_scaled, metric='euclidean')
    distance_matrix[start_row:end_row] = distance_chunk

# Perform agglomerative clustering
num_clusters = 30
clustering = AgglomerativeClustering(n_clusters=num_clusters, affinity='precomputed', linkage='average')
item_clusters = clustering.fit_predict(distance_matrix)

# Convert item IDs back to original
item_user_matrix_sparse = item_user_matrix_sparse.toarray()  # Convert sparse matrix back to dense for processing
item_user_matrix_sparse = pd.DataFrame(item_user_matrix_sparse, index=trainset_df['anime_id'].astype('category').cat.categories)
item_user_matrix_sparse['cluster'] = item_clusters

# Evaluation
silhouette_avg = silhouette_score(item_user_matrix_scaled, item_clusters)

print(f"Silhouette Score: {silhouette_avg}")

# Function to get recommendations
def get_recommendations(anime_id, n_recommendations):
    # Find the cluster of the given anime
    cluster = item_user_matrix_sparse.loc[anime_id]['cluster']

    # Get all animes in the same cluster
    similar_animes = item_user_matrix_sparse[item_user_matrix_sparse['cluster'] == cluster].index.tolist()

    # Exclude the input anime
    similar_animes.remove(anime_id)

    return similar_animes[:n_recommendations]

# Example: Get recommendations for a specific anime
anime_id = 1  # Example anime_id
n_recommendations = 5
recommendations = get_recommendations(anime_id, n_recommendations)
print(f"Recommendations for anime_id {anime_id}: {recommendations}")

# We'll use the testset to evaluate the RMSE of the model.
from surprise import KNNBasic
from surprise import Dataset, Reader, accuracy

# Define the similarity options
sim_options = {
    'name': 'cosine',
    'user_based': False
}

# Train a basic KNN model for item-based collaborative filtering
algo = KNNBasic(sim_options=sim_options)
algo.fit(trainset)

# Predict ratings for the testset
predictions = algo.test(testset)

# Calculate RMSE
rmse = accuracy.rmse(predictions)
print(f"RMSE: {rmse}")

"""#### Hyperparameter Tuning"""

from surprise import KNNBasic
from surprise.model_selection import GridSearchCV

# Hyperparameter tuning using GridSearchCV with reduced parameter grid and cross-validation folds
param_grid = {
    'k': [20, 30],  # Reduced range for `k`
    'min_k': [2, 3],  # Reduced range for `min_k`
    'sim_options': {
        'name': ['cosine'],  # Reduced similarity measures
        'user_based': [False]  # item-based filtering
    }
}

# Use fewer folds for cross-validation to reduce memory usage
gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=2, n_jobs=1)
gs.fit(dataset)

# Best parameters
print("Best RMSE score: ", gs.best_score['rmse'])
print("Best parameters: ", gs.best_params['rmse'])

# Train the best model
best_algo = gs.best_estimator['rmse']
best_algo.fit(trainset)

# Predict ratings for the testset
predictions = best_algo.test(testset)

# Calculate RMSE
rmse = accuracy.rmse(predictions)
print(f"RMSE: {rmse}")

"""#### Hybrid Recommender System"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import pairwise_distances
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from scipy.sparse import csr_matrix


# Content-based part
anime_df_cleaned['genre'] = anime_df_cleaned['genre'].fillna('Unknown')
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(anime_df_cleaned['genre'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# User-based part
# Drop duplicates by aggregating ratings (e.g., taking the mean)
train_df_cleaned = train_df_cleaned.groupby(['user_id', 'anime_id']).rating.mean().reset_index()
user_item_matrix = train_df_cleaned.pivot(index='user_id', columns='anime_id', values='rating')
scaler = StandardScaler(with_mean=False)
user_item_matrix_scaled = scaler.fit_transform(user_item_matrix)
user_similarity = 1 - pairwise_distances(user_item_matrix_scaled, metric='cosine')

# Combine both approaches
def get_hybrid_recommendations(user_id, anime_id, top_n=10):
    # Content-based recommendations
    idx = anime_df_cleaned[anime_df_cleaned['anime_id'] == anime_id].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]
    content_recommendations = [anime_df_cleaned.iloc[i[0]]['anime_id'] for i in sim_scores]

    # User-based recommendations
    if user_id not in user_item_matrix.index:
        return content_recommendations  # Fallback to content-based recommendations if user is not found

    user_idx = user_item_matrix.index.get_loc(user_id)
    similar_users = user_similarity[user_idx]
    similar_users_idx = similar_users.argsort()[-top_n:]
    similar_anime = user_item_matrix.iloc[similar_users_idx].mean().sort_values(ascending=False)
    user_based_recommendations = similar_anime.index.tolist()

    # Combine recommendations
    hybrid_recommendations = list(set(content_recommendations + user_based_recommendations))
    return hybrid_recommendations[:top_n]

# Example: Get hybrid recommendations for a specific user and anime
user_id = 1  # Example user_id
anime_id = 1  # Example anime_id
recommendations = get_hybrid_recommendations(user_id, anime_id)
print("Hybrid Recommendations:")
print(recommendations)

"""## 9 Conclusion <a class="anchor" id="chapter10"></a>

<b> Summary of Key Findings:</b> The analysis revealed that SVM with an accuracy of 99.89% rounded to 100  and an F1-score of 0.9999 rounded to 1.0 outperformed other models in classifying news articles across multiple domains. Key topics such as politics and technology were accurately classified using this model. However all the model were able to predict unseen data very well.

<b> Evaluation of the Methodology:</b>Our approach leveraged a combination of TF-IDF vectorization and SVM, which proved effective in handling multi-domain classification. However, the reliance on labeled data and the computational cost of SVM for large datasets were notable limitations.

<b> Implications of the Findings:</b>These findings suggest that implementing SVM for news classification can enhance content recommendation systems and improve user engagement by delivering more relevant content. This can potentially increase platform retention and user satisfaction.

<b> Suggestions for Future Work:</b>Future research could explore ensemble methods to further boost classification accuracy and scalability, especially for real-time applications. Additionally, integrating deep learning techniques like BERT for better understanding of contextual nuances in news articles could be beneficial.

<b> Reflection on the Data Source and Quality:</b>The dataset sourced from reputable news outlets provided a diverse range of articles, ensuring broad coverage across domains. However, challenges included occasional bias in reporting and the need for continuous data augmentation to maintain model robustness.

<b> Concluding Thoughts:</b>Overall, our study demonstrates the efficacy of SVM in multi-domain news classification while highlighting opportunities for refinement and innovation. By addressing methodological gaps and leveraging evolving technologies, future work can advance the field of automated content classification and its practical applications.

## 10. References <a class="anchor" id="chapter11"></a>

- Scikit-learn Documentation: Provides detailed guides and examples for using machine learning models and tuning hyperparameters with Scikit-learn.
- Towards Data Science: Offers a wide range of articles, tutorials, and discussions on data science and machine learning topics, catering to both beginners and advanced practitioners.
- Machine Learning Mastery: Focuses on practical tutorials and tips for mastering machine learning techniques, with a hands-on approach to implementing algorithms and tuning models.

- Kaggle: A platform for data science competitions and datasets, featuring community-contributed notebooks and discussions that showcase various machine learning methods and strategies.

- Coursera: Offers online courses on machine learning and data science from top universities and instructors, providing structured learning paths and hands-on projects.

These resources serve as valuable tools for learning, implementing, and mastering machine learning models, hyperparameter tuning techniques, and best practices in data science. They cater to diverse learning styles and levels of expertise, making them essential references for anyone interested in advancing their skills in these fields.
"""